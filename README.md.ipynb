{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Project\n",
    "\n",
    "## Exploration of Subway Acessibility, Real Estate and 311 Reports Across NYC (2020)\n",
    "\n",
    "Group Members: Ryan Eccleston & Alison Sadel\n",
    "* Context: For this project, we decided to extract, transform and load a datset with information on New York subway stations, residential real estate sales and 311 complaints. Once the ETL process is complete, we ultimately would like to explore socio-economic behaviors and potential disparities within New York City, segmented by zip code for 2020. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sources\n",
    "* City of New York Subway Station Data\n",
    "    * https://data.cityofnewyork.us/Transportation/Subway-Stations/arq3-7z49\n",
    "* New York Government Statistics\n",
    "    * https://www.health.ny.gov/statistics/cancer/registry/appendix/neighborhoods.htm\n",
    "* Socrata 311 API\n",
    "    * https://nycopendata.socrata.com/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9\n",
    "* Residential Real Estate Sales 2020\n",
    "    * http://compass.com/agents (restricted access)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Process\n",
    "* Subway Station Data\n",
    "    * Leveraged Google’s Reverse Geocod[e/ing] API  to transform point objects to an address where the street address/cross street and zip code was extracted.\n",
    "* Heath NYC - NY Neighborhood ZIP Code Definitions\n",
    "    * Scraped NY borough-neighborhood-zip code delineation from a table to separate lists.\n",
    "* 311 Complaints Data\n",
    "    * Modules required to create the API request for Socrata were Sodapy, json, pprint. \n",
    "    * Created a url path to request 311 Data, segmented for January 1st 2020 - February 12th 2021 with a 200k limit. After making the API call the 200k rows of data were loaded into a DataFrame.\n",
    "* Housing Data\n",
    "    * Filtered by borough and month/year and manually would select all 120 results for each page and export to google sheets. After creating the CSVs segmented by borough, we used a pd.concat( ) to merge the Dataframess.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Limitations & Future Considerations\n",
    "* Subway Station Data\n",
    "    * Though this is an exhaustive list of subway stations, it does not provide any immediate indication of the station’s status or accessibility accommodations, only a link (under construction, etc.) \n",
    "* Heath NYC - NY Neighborhood ZIP Code Definitions\n",
    "    * The neighborhoods column is not comprehensive (ex: Astoria is not missing). With Housing Data with a field for neighborhood and the nature of the dataset neighborhood is likely to be more accurate, although still a relative term.\n",
    "    * Example: what I consider Upper West Side may not be your definitition of the Upper West Side).\n",
    "* 311 Complaints Data\n",
    "    * Memory becomes relevant when processing the 311 API. We segmented for 2020 which contains 44M rows of data. We opted to make a request with a 200,000 limit to align with system capabilities.\n",
    "    * To determine if the 200k sample size is sufficient for future data exploration, it may be helpful to run an independent T-Test.\n",
    "        \n",
    "* Housing Data\n",
    "    * Due to a load error on a specific page, 120 of the Brooklyn records were corrupted and unable to be extracted. \n",
    "    * New York Real Estate has no standardized MLS so there likely many additional transactions that occured in 2020 not reflected (non-REBNY, off-market). \n",
    "    * The data retrieval for the real estate dataset was extremely manual. In the future, using Splinter and Beautiful soup may be a future option to maintain the dataset over time.\n",
    "    * One data capture issue we had was due to no standardized MLS (impacts reporting), there were many NaN values across the columns. With the intent to merge all data on zipcode, we ultimately dropped 9,634 columns. An alternative solution would be to run a Google Place API against the dataset, using the addresses to get latitude and longitude and then fill in those missing zipcode values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Data\n",
    "* Subway Station Data\n",
    "    * Removed and renamed columns.\n",
    "    * Checked for data type issues using a for loop to determine .dtype - no additional actions required.\n",
    "    * Adjusted index to begin at 1 using .index +=1\n",
    "* 311 Complaints Data\n",
    "    * Removed unnecessary columns, reducing the number of columns from 41 to 24.\n",
    "    * Migrated all data from columns containing the value ‘cross street’ to intersection values to  account for missing values using fillna( ) function. Once the data was merged between the 4 columns, we dropped the 2 redundant ‘cross-street’ columns. After removing the duplicative columns the 311 dataset contained 22 columns.\n",
    "    * Renamed column headers to merge on ‘zipcode’ and reduced character length of headers for easier horizontal reading.\n",
    "    * Converted all columns containing string values to lowercase for future data exploration using apply.map( ) function.\n",
    "    * With the intent to merge dataframes on ‘zipcode’, all rows without a zipcode specified were dropped using the dropna( ) function. Dropping NaNs across rows removed 6,974 rows.\n",
    "    * The original dataset was composed of 200,000 incident reports (rows) and 41 fields (columns). The cleaned 311 Dataframe contains 22 columns and 193026.\n",
    "* Housing Data\n",
    "    * Removed unnecessary columns, reducing the number of columns from 41 to 17.\n",
    "    * Renamed all column headers to merge on 'zipcode', made all headers lowercase, removed spaces and provided clearer definitiions (ex: 'DOM' became 'days on market')\n",
    "    * Checked data types using dtypes( ) function. Converted ppsf to float by removing $ sign with a replace( ) function and converted ppsf to float using astype( ) function for future data exploration.\n",
    "    * With the intent to merge dataframes on ‘zipcode’, all rows without a zipcode specified were dropped using the dropna( ) function. Dropping NaNs across rows removed 9,634 columns.\n",
    "    * Converted all columns containing string values to lowercase for future data exploration using apply.map( ) function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources \n",
    "* 311 Complaints Data (311_clean.zip)\n",
    "    * Prior to transform process, the original dataset was composed of 200,000 incident reports (rows) and 41 fields (columns). The 200,000 sample size (of 44M records), documents 311 complaints for 2020.\n",
    "    * The cleaned 311 Dataframe contains 22 columns and 193026 rows.\n",
    "\n",
    "* Housing Data (real_estate_clean.csv)\n",
    "    * Prior to transform process, the original dataset was composed of 49,876 residential real estate sales (rows) and 41 fields (columns) from 2020. \n",
    "    * The cleaned Real Estate Dataframe contains 17 columns and 40,242 rows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
